{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of predicting a numeric variable like a linear regression, logistic regression predicts the probability (between 0 and 1) of an event.\n",
    "\n",
    "To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a 'yes' (in this case, the flight being late), if it's below, you classify it as a 'no'!\n",
    "\n",
    "You'll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that's not estimated from the data, but rather is supplied by the user to maximize performance. For this course it's not necessary to understand the mathematics behind all of these values - what's important is that you'll try out a few different choices and pick the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you supply hyperparameters?\n",
    "- They improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the LogisticRegression class from pyspark.ml.classification.\n",
    "- Create a LogisticRegression called lr by calling LogisticRegression() with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import LogisticRegression\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# # Create a LogisticRegression Estimator\n",
    "# lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method of estimating the model's performance on unseen data (like your test DataFrame).\n",
    "\n",
    "It works by splitting the training data into a few different partitions. The exact number is up to you. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does cross validation allow you to estimate?\n",
    "- The model's error on held out data (test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the submodule pyspark.ml.evaluation as evals.\n",
    "- Create evaluator by calling evals.BinaryClassificationEvaluator() with the argument metricName=\"areaUnderROC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the evaluation submodule\n",
    "# import pyspark.ml.evaluation as evals\n",
    "\n",
    "# # Create a BinaryClassificationEvaluator\n",
    "# evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the submodule pyspark.ml.tuning under the alias tune.\n",
    "- Call the class constructor ParamGridBuilder() with no arguments. Save this as grid.\n",
    "- Call the .addGrid() method on grid with lr.regParam as the first argument and np.arange(0, .1, .01) as the second argument. This second call is a function from the numpy module (imported as np) that creates a list of numbers from 0 to .1, incrementing by .01. Overwrite grid with the result.\n",
    "- Update grid again by calling the .addGrid() method a second time create a grid for lr.elasticNetParam that includes only the values [0, 1].\n",
    "- Call the .build() method on grid and overwrite it with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the tuning submodule\n",
    "# import pyspark.ml.tuning as tune\n",
    "\n",
    "# # Create the parameter grid\n",
    "# grid = tune.ParamGridBuilder()\n",
    "\n",
    "# # Add the hyperparameter\n",
    "# grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "# grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# # Build the grid\n",
    "# grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a CrossValidator by calling tune.CrossValidator() with the arguments:\n",
    "- estimator=lr\n",
    "- estimatorParamMaps=grid\n",
    "- evaluator=evaluator\n",
    "- Name this object cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the CrossValidator\n",
    "# cv = tune.CrossValidator(estimator=lr,\n",
    "#                estimatorParamMaps=grid,\n",
    "#                evaluator=evaluator\n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create best_lr by calling lr.fit() on the training data.\n",
    "- Print best_lr to verify that it's an object of the LogisticRegressionModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call lr.fit()\n",
    "# best_lr = lr.fit(training)\n",
    "\n",
    "# # Print best_lr\n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closer the AUC is to one (1), the better the model is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've created a perfect binary classification model, what would the AUC be?\n",
    "- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use your model to generate predictions by applying best_lr.transform() to the test data. Save this as test_results.\n",
    "- Call evaluator.evaluate() on test_results to compute the AUC. Print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the model to predict the test set\n",
    "# test_results = best_lr.transform(test)\n",
    "\n",
    "# # Evaluate the predictions\n",
    "# print(evaluator.evaluate(test_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
