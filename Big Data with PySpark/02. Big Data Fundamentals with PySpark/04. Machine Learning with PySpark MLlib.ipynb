{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of data structures does pyspark.mllib built-in library support in Spark?\n",
    "- pyspark.mllib is the builtin library for RDD-based API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import pyspark.mllib recommendation submodule and Alternating Least Squares class.\n",
    "- Import pyspark.mllib classification submodule and Logistic Regression with LBFGS class.\n",
    "- Import pyspark.mllib clustering submodule and kmeans class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the library for ALS\n",
    "# from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# # Import the library for Logistic Regression\n",
    "# from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# # Import the library for Kmeans\n",
    "# from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the ratings.csv dataset into an RDD.\n",
    "- Split the RDD using , as a delimiter.\n",
    "- For each line of the RDD, using Rating() class create a tuple of userID, productID, rating.\n",
    "- Randomly split the data into training data and test data (0.8 and 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data into RDD\n",
    "# data = sc.textFile(file_path)\n",
    "\n",
    "# # Split the RDD \n",
    "# ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# # Transform the ratings RDD \n",
    "# ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# # Split the data into training and test\n",
    "# training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train ALS algorithm with training data and configured parameters (rank = 10 and iterations = 10).\n",
    "- Drop the rating column in the test data.\n",
    "- Test the model by predicting the rating from the test data.\n",
    "- Return a list of two rows of the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the ALS model on the training data\n",
    "# model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# # Drop the ratings column \n",
    "# testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# # Predict the model  \n",
    "# predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# # Return the first 2 rows of the RDD\n",
    "# predictions.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Organize ratings RDD to make ((user, product), rating).\n",
    "- Organize predictions RDD to make ((user, product), rating).\n",
    "- Join the prediction RDD with the ratings RDD.\n",
    "- Evaluate the model using MSE between original rating and predicted rating and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare ratings data\n",
    "# rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# # Prepare predictions data\n",
    "# preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# # Join the ratings data with predictions data\n",
    "# rates_and_preds = rates.join(preds)\n",
    "\n",
    "# # Calculate and print MSE\n",
    "# MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "# print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create two RDDS, one for 'spam' and one for 'non-spam (ham)'.\n",
    "- Split each email in 'spam' and 'non-spam' RDDs into words.\n",
    "- Print the first element in the split RDD of both 'spam' and 'non-spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the datasets into RDDs\n",
    "# spam_rdd = sc.textFile(file_path_spam)\n",
    "# non_spam_rdd = sc.textFile(file_path_non_spam)\n",
    "\n",
    "# # Split the email messages into words\n",
    "# spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "# non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# # Print the first element in the split RDD\n",
    "# print(\"The first element in spam_words is\", spam_words.first())\n",
    "# print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a HashingTF() instance to map email text to vectors of 200 features.\n",
    "- Each message in 'spam' and 'non-spam' datasets are split into words, and each word is mapped to one feature.\n",
    "- Label the features: 1 for spam, 0 for non-spam.\n",
    "- Combine both the spam and non-spam samples into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a HashingTf instance with 200 features\n",
    "# tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# # Map each word to one feature\n",
    "# spam_features = tf.transform(spam_words)\n",
    "# non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# # Label the features: 1 for spam, 0 for non-spam\n",
    "# spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "# non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# # Combine the two datasets\n",
    "# samples = spam_samples.join(non_spam_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the combined data into training and test datasets in 80:20 ratio.\n",
    "- Train the Logistic Regression model with the training dataset.\n",
    "- Create a prediction label from the trained model on the test dataset.\n",
    "- Combine the labels in the test dataset with the labels in the prediction dataset.\n",
    "- Calculate the accuracy of the trained model using original and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data into training and testing\n",
    "# train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# # Train the model\n",
    "# model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# # Create a prediction label from the test data\n",
    "# predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# # Combine original labels with the predicted labels\n",
    "# labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "\n",
    "# # Check the accuracy of the model on the test data\n",
    "# accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "# print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the 5000_points dataset into an RDD named clusterRDD.\n",
    "- Transform the clusterRDD by splitting the lines based on the tab (\"\\t\").\n",
    "- Transform the split RDD to create a list of integers for the two columns.\n",
    "- Confirm that there are 5000 rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the dataset into an RDD\n",
    "# clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# # Split the RDD based on tab\n",
    "# rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# # Transform the split RDD by creating a list of integers\n",
    "# rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# # Count the number of rows in RDD \n",
    "# print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the KMeans model with clusters from 13 to 16 and print the WSSSE for each cluster.\n",
    "- Train the KMeans model again with the best k.\n",
    "- Get the Cluster Centers (centroids) of KMeans model trained with the best k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import sqrt\n",
    "# def error(point):\n",
    "# center = model.centers[model.predict(point)]\n",
    "# return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model with clusters from 13 to 16 and compute WSSSE\n",
    "# for clst in range(13, 17):\n",
    "#     model = KMeans.train(rdd_split_int, clst, seed=1)\n",
    "#     WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "#     print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
    "\n",
    "# # Train the model again with the best k\n",
    "# model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# # Get cluster centers\n",
    "# cluster_centers = model.clusterCenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the rdd_split_int RDD to a Spark DataFrame, then to a pandas DataFrame.\n",
    "- Create a pandas DataFrame from the cluster_centers list.\n",
    "- Create a scatter plot from the pandas DataFrame of raw data (rdd_split_int_df_pandas) and overlay that with a scatter plot from the Pandas DataFrame of centroids (cluster_centers_pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert rdd_split_int RDD into Spark DataFrame and then to Pandas DataFrame\n",
    "# rdd_split_int_df_pandas = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"]).toPandas()\n",
    "\n",
    "# # Convert cluster_centers to a pandas DataFrame\n",
    "# cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# # Create an overlaid scatter plot of clusters and centroids\n",
    "# plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "# plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
