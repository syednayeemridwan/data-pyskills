{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an RDD named RDD from a list of words.\n",
    "- Confirm the object created is RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an RDD from a list of words\n",
    "# RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# # Print out the type of the created object\n",
    "# print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the file_path in the PySpark shell.\n",
    "- Create an RDD named fileRDD from a file_path.\n",
    "- Print the type of the fileRDD created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the file_path\n",
    "# print(\"The file_path is\", file_path)\n",
    "\n",
    "# # Create a fileRDD from file_path\n",
    "# fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# # Check the type of fileRDD\n",
    "# print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the number of partitions that support fileRDD RDD.\n",
    "- Create an RDD named fileRDD_part from the file path but create 5 partitions.\n",
    "- Confirm the number of partitions in the new fileRDD_part RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the number of partitions in fileRDD\n",
    "# print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# # Create a fileRDD_part from file_path with 5 partitions\n",
    "# fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# # Check the number of partitions in fileRDD_part\n",
    "# print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create map() transformation that cubes all of the numbers in numbRDD.\n",
    "- Collect the results in a numbers_all variable.\n",
    "- Print the output from numbers_all variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create map() transformation to cube numbers\n",
    "# cubedRDD = numbRDD.map(lambda x: x ** 3)\n",
    "\n",
    "# # Collect the results\n",
    "# numbers_all = cubedRDD.collect()\n",
    "\n",
    "# # Print the numbers from numbers_all\n",
    "# for numb in numbers_all:\n",
    "# \tprint(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create filter() transformation to select the lines containing the keyword Spark.\n",
    "- How many lines in fileRDD_filter contains the keyword Spark?\n",
    "- Print the first four lines of the resulting RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the fileRDD to select lines with Spark keyword\n",
    "# fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# # How many lines are there in fileRDD?\n",
    "# print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# # Print the first four lines of fileRDD\n",
    "# for line in fileRDD_filter.take(4): \n",
    "#   print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).\n",
    "- Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.\n",
    "- Collect the contents of pair RDD Rdd_Reduced and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create PairRDD Rdd with key value pairs\n",
    "# Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "# # Apply reduceByKey() operation on Rdd\n",
    "# Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# # Iterate over the result and print the output\n",
    "# for num in Rdd_Reduced.collect(): \n",
    "#   print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort the Rdd_Reduced RDD using the key in descending order.\n",
    "- Collect the contents and iterate to print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the reduced RDD with the key by descending order\n",
    "# Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# # Iterate over the result and retrieve all the elements of the RDD\n",
    "# for num in Rdd_Reduced_Sort.collect():\n",
    "#   print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the unique keys and assign the result to a variable total.\n",
    "- What is the type of total?\n",
    "- Iterate over the total and print the keys and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the unique keys\n",
    "# total = Rdd.countByKey()\n",
    "\n",
    "# # What is the type of total?\n",
    "# print(\"The type of total is\", type(total))\n",
    "\n",
    "# # Iterate over the total and print the output\n",
    "# for k, v in total.items(): \n",
    "#   print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an RDD called baseRDD that reads lines from file_path.\n",
    "- Transform the baseRDD into a long list of words and create a new splitRDD.\n",
    "- Count the total words in splitRDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a baseRDD from the file path\n",
    "# baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# # Split the lines of baseRDD into words\n",
    "# splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "\n",
    "# # Count the total number of words\n",
    "# print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the words in splitRDD in lower case and then remove stop words from stop_words curated list.\n",
    "- Create a pair RDD tuple containing the word and the number 1 from each word element in splitRDD.\n",
    "- Get the count of the number of occurrences of each word (word frequency) in the pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "# splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# # Create a tuple of the word and 1 \n",
    "# splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# # Count of the number of occurences of each word\n",
    "# resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the first 10 words and their frequencies from the resultRDD RDD.\n",
    "- Swap the keys and values in the resultRDD.\n",
    "- Sort the keys according to descending order.\n",
    "- Print the top 10 most frequent words and their frequencies from the sorted RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the first 10 words and their frequencies from the input RDD\n",
    "# for word in resultRDD.take(10):\n",
    "# \tprint(word)\n",
    "\n",
    "# # Swap the keys and values from the input RDD\n",
    "# resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# # Sort the keys in descending order\n",
    "# resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# # Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "# for word in resultRDD_swap_sort.take(10):\n",
    "# \tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
